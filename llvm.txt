(array(int8, 1d, C),)//////////////; ModuleID = 'sum2d'
source_filename = "<string>"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-pc-windows-msvc"

@.const.sum2d = internal constant [6 x i8] c"sum2d\00"
@".const.Fatal error: missing _dynfunc.Closure" = internal constant [38 x i8] c"Fatal error: missing _dynfunc.Closure\00"
@PyExc_RuntimeError = external global i8
@".const.missing Environment" = internal constant [20 x i8] c"missing Environment\00"
@_Py_NoneStruct = external global i8

; Function Attrs: norecurse nounwind
define i32 @"_ZN8__main__9sum2d$241E5ArrayIaLi1E1C7mutable7alignedE"(i8** noalias nocapture %retptr, { i8*, i32 }** noalias nocapture readnone %excinfo, i8* noalias nocapture readnone %env, i8* nocapture readnone %arg.arr.0, i8* nocapture readnone %arg.arr.1, i64 %arg.arr.2, i64 %arg.arr.3, i8* nocapture %arg.arr.4, i64 %arg.arr.5.0, i64 %arg.arr.6.0) local_unnamed_addr #0 {
entry:
  %.78 = icmp sgt i64 %arg.arr.2, 0
  br i1 %.78, label %B16.lr.ph, label %B67

B16.lr.ph:                                        ; preds = %entry
  %0 = xor i64 %arg.arr.2, -1
  %1 = icmp sgt i64 %0, -2
  %smax = select i1 %1, i64 %0, i64 -2
  %2 = add i64 %smax, %arg.arr.2
  %3 = add i64 %2, 2
  %min.iters.check = icmp ult i64 %3, 32
  br i1 %min.iters.check, label %B16.preheader, label %vector.ph

vector.ph:                                        ; preds = %B16.lr.ph
  %n.vec = and i64 %3, -32
  %ind.end = sub i64 %arg.arr.2, %n.vec
  %4 = add i64 %n.vec, -32
  %5 = lshr exact i64 %4, 5
  %6 = add nuw nsw i64 %5, 1
  %xtraiter = and i64 %6, 3
  %7 = icmp ult i64 %4, 96
  br i1 %7, label %middle.block.unr-lcssa, label %vector.ph.new

vector.ph.new:                                    ; preds = %vector.ph
  %8 = add i64 %xtraiter, -1
  %9 = sub i64 %8, %5
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph.new
  %lsr.iv38 = phi i64 [ %lsr.iv.next39, %vector.body ], [ %9, %vector.ph.new ]
  %index = phi i64 [ 0, %vector.ph.new ], [ %index.next.3, %vector.body ]
  %vec.ind = phi <32 x i64> [ <i64 0, i64 1, i64 2, i64 3, i64 4, i64 5, i64 6, i64 7, i64 8, i64 9, i64 10, i64 11, i64 12, i64 13, i64 14, i64 15, i64 16, i64 17, i64 18, i64 19, i64 20, i64 21, i64 22, i64 23, i64 24, i64 25, i64 26, i64 27, i64 28, i64 29, i64 30, i64 31>, %vector.ph.new ], [ %vec.ind.next.3, %vector.body ]
  %sunkaddr = getelementptr i8, i8* %arg.arr.4, i64 %index
  %10 = bitcast i8* %sunkaddr to <32 x i8>*
  %wide.load = load <32 x i8>, <32 x i8>* %10, align 1
  %11 = zext <32 x i8> %wide.load to <32 x i64>
  %12 = add <32 x i64> %vec.ind, %11
  %13 = trunc <32 x i64> %12 to <32 x i8>
  %14 = icmp sgt <32 x i8> %13, <i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100>
  %15 = add <32 x i64> %12, <i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156>
  %16 = select <32 x i1> %14, <32 x i64> %15, <32 x i64> %12
  %17 = trunc <32 x i64> %16 to <32 x i8>
  %sunkaddr40 = getelementptr i8, i8* %arg.arr.4, i64 %index
  %18 = bitcast i8* %sunkaddr40 to <32 x i8>*
  store <32 x i8> %17, <32 x i8>* %18, align 1
  %vec.ind.next = add <32 x i64> %vec.ind, <i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32>
  %sunkaddr41 = getelementptr i8, i8* %arg.arr.4, i64 %index
  %sunkaddr42 = getelementptr i8, i8* %sunkaddr41, i64 32
  %19 = bitcast i8* %sunkaddr42 to <32 x i8>*
  %wide.load.1 = load <32 x i8>, <32 x i8>* %19, align 1
  %20 = zext <32 x i8> %wide.load.1 to <32 x i64>
  %21 = add <32 x i64> %vec.ind.next, %20
  %22 = trunc <32 x i64> %21 to <32 x i8>
  %23 = icmp sgt <32 x i8> %22, <i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100>
  %24 = add <32 x i64> %21, <i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156>
  %25 = select <32 x i1> %23, <32 x i64> %24, <32 x i64> %21
  %26 = trunc <32 x i64> %25 to <32 x i8>
  %sunkaddr43 = getelementptr i8, i8* %arg.arr.4, i64 %index
  %sunkaddr44 = getelementptr i8, i8* %sunkaddr43, i64 32
  %27 = bitcast i8* %sunkaddr44 to <32 x i8>*
  store <32 x i8> %26, <32 x i8>* %27, align 1
  %vec.ind.next.1 = add <32 x i64> %vec.ind, <i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64>
  %sunkaddr45 = getelementptr i8, i8* %arg.arr.4, i64 %index
  %sunkaddr46 = getelementptr i8, i8* %sunkaddr45, i64 64
  %28 = bitcast i8* %sunkaddr46 to <32 x i8>*
  %wide.load.2 = load <32 x i8>, <32 x i8>* %28, align 1
  %29 = zext <32 x i8> %wide.load.2 to <32 x i64>
  %30 = add <32 x i64> %vec.ind.next.1, %29
  %31 = trunc <32 x i64> %30 to <32 x i8>
  %32 = icmp sgt <32 x i8> %31, <i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100>
  %33 = add <32 x i64> %30, <i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156>
  %34 = select <32 x i1> %32, <32 x i64> %33, <32 x i64> %30
  %35 = trunc <32 x i64> %34 to <32 x i8>
  %sunkaddr47 = getelementptr i8, i8* %arg.arr.4, i64 %index
  %sunkaddr48 = getelementptr i8, i8* %sunkaddr47, i64 64
  %36 = bitcast i8* %sunkaddr48 to <32 x i8>*
  store <32 x i8> %35, <32 x i8>* %36, align 1
  %vec.ind.next.2 = add <32 x i64> %vec.ind, <i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96>
  %sunkaddr49 = getelementptr i8, i8* %arg.arr.4, i64 %index
  %sunkaddr50 = getelementptr i8, i8* %sunkaddr49, i64 96
  %37 = bitcast i8* %sunkaddr50 to <32 x i8>*
  %wide.load.3 = load <32 x i8>, <32 x i8>* %37, align 1
  %38 = zext <32 x i8> %wide.load.3 to <32 x i64>
  %39 = add <32 x i64> %vec.ind.next.2, %38
  %40 = trunc <32 x i64> %39 to <32 x i8>
  %41 = icmp sgt <32 x i8> %40, <i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100>
  %42 = add <32 x i64> %39, <i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156>
  %43 = select <32 x i1> %41, <32 x i64> %42, <32 x i64> %39
  %44 = trunc <32 x i64> %43 to <32 x i8>
  %sunkaddr51 = getelementptr i8, i8* %arg.arr.4, i64 %index
  %sunkaddr52 = getelementptr i8, i8* %sunkaddr51, i64 96
  %45 = bitcast i8* %sunkaddr52 to <32 x i8>*
  store <32 x i8> %44, <32 x i8>* %45, align 1
  %index.next.3 = add i64 %index, 128
  %vec.ind.next.3 = add <32 x i64> %vec.ind, <i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128>
  %lsr.iv.next39 = add i64 %lsr.iv38, 4
  %niter.ncmp.3 = icmp eq i64 %lsr.iv.next39, 0
  br i1 %niter.ncmp.3, label %middle.block.unr-lcssa, label %vector.body, !llvm.loop !0

middle.block.unr-lcssa:                           ; preds = %vector.body, %vector.ph
  %index.unr = phi i64 [ 0, %vector.ph ], [ %index.next.3, %vector.body ]
  %vec.ind.unr = phi <32 x i64> [ <i64 0, i64 1, i64 2, i64 3, i64 4, i64 5, i64 6, i64 7, i64 8, i64 9, i64 10, i64 11, i64 12, i64 13, i64 14, i64 15, i64 16, i64 17, i64 18, i64 19, i64 20, i64 21, i64 22, i64 23, i64 24, i64 25, i64 26, i64 27, i64 28, i64 29, i64 30, i64 31>, %vector.ph ], [ %vec.ind.next.3, %vector.body ]
  %lcmp.mod = icmp eq i64 %xtraiter, 0
  br i1 %lcmp.mod, label %middle.block, label %vector.body.epil.preheader

vector.body.epil.preheader:                       ; preds = %middle.block.unr-lcssa
  %scevgep9 = getelementptr i8, i8* %arg.arr.4, i64 %index.unr
  %46 = sub i64 0, %xtraiter
  br label %vector.body.epil

vector.body.epil:                                 ; preds = %vector.body.epil, %vector.body.epil.preheader
  %lsr.iv13 = phi i64 [ %lsr.iv.next14, %vector.body.epil ], [ %46, %vector.body.epil.preheader ]
  %lsr.iv10 = phi i8* [ %scevgep11, %vector.body.epil ], [ %scevgep9, %vector.body.epil.preheader ]
  %vec.ind.epil = phi <32 x i64> [ %vec.ind.unr, %vector.body.epil.preheader ], [ %vec.ind.next.epil, %vector.body.epil ]
  %lsr.iv1012 = bitcast i8* %lsr.iv10 to <32 x i8>*
  %wide.load.epil = load <32 x i8>, <32 x i8>* %lsr.iv1012, align 1
  %47 = zext <32 x i8> %wide.load.epil to <32 x i64>
  %48 = add <32 x i64> %vec.ind.epil, %47
  %49 = trunc <32 x i64> %48 to <32 x i8>
  %50 = icmp sgt <32 x i8> %49, <i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100>
  %51 = add <32 x i64> %48, <i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156>
  %52 = select <32 x i1> %50, <32 x i64> %51, <32 x i64> %48
  %53 = trunc <32 x i64> %52 to <32 x i8>
  store <32 x i8> %53, <32 x i8>* %lsr.iv1012, align 1
  %vec.ind.next.epil = add <32 x i64> %vec.ind.epil, <i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32>
  %scevgep11 = getelementptr i8, i8* %lsr.iv10, i64 32
  %lsr.iv.next14 = add nsw i64 %lsr.iv13, 1
  %epil.iter.cmp = icmp eq i64 %lsr.iv.next14, 0
  br i1 %epil.iter.cmp, label %middle.block, label %vector.body.epil, !llvm.loop !3

middle.block:                                     ; preds = %vector.body.epil, %middle.block.unr-lcssa
  %cmp.n = icmp eq i64 %3, %n.vec
  br i1 %cmp.n, label %B67, label %B16.preheader

B16.preheader:                                    ; preds = %middle.block, %B16.lr.ph
  %.68.07.ph = phi i64 [ %arg.arr.2, %B16.lr.ph ], [ %ind.end, %middle.block ]
  %.66.06.ph = phi i64 [ 0, %B16.lr.ph ], [ %n.vec, %middle.block ]
  %54 = add i64 %.68.07.ph, 1
  br label %B16

B16:                                              ; preds = %B16.preheader, %B16
  %lsr.iv = phi i64 [ %54, %B16.preheader ], [ %lsr.iv.next, %B16 ]
  %.66.06 = phi i64 [ %.130, %B16 ], [ %.66.06.ph, %B16.preheader ]
  %.130 = add nuw nsw i64 %.66.06, 1
  %scevgep = getelementptr i8, i8* %arg.arr.4, i64 %.66.06
  %.192 = load i8, i8* %scevgep, align 1
  %.1981 = zext i8 %.192 to i64
  %.199 = add i64 %.66.06, %.1981
  %tmp = trunc i64 %.199 to i8
  %.278 = icmp sgt i8 %tmp, 100
  %.329 = add i64 %.199, 156
  %.329..199 = select i1 %.278, i64 %.329, i64 %.199
  %storemerge = trunc i64 %.329..199 to i8
  store i8 %storemerge, i8* %scevgep, align 1
  %lsr.iv.next = add i64 %lsr.iv, -1
  %.117 = icmp sgt i64 %lsr.iv.next, 1
  br i1 %.117, label %B16, label %B67, !llvm.loop !5

B67:                                              ; preds = %B16, %middle.block, %entry
  store i8* null, i8** %retptr, align 8
  ret i32 0
}

define i8* @"_ZN7cpython8__main__9sum2d$241E5ArrayIaLi1E1C7mutable7alignedE"(i8* %py_closure, i8* %py_args, i8* nocapture readnone %py_kws) local_unnamed_addr {
entry:
  %.5 = alloca i8*, align 8
  %.6 = call i32 (i8*, i8*, i64, i64, ...) @PyArg_UnpackTuple(i8* %py_args, i8* getelementptr inbounds ([6 x i8], [6 x i8]* @.const.sum2d, i64 0, i64 0), i64 1, i64 1, i8** nonnull %.5)
  %.7 = icmp eq i32 %.6, 0
  %.30 = alloca { i8*, i8*, i64, i64, i8*, [1 x i64], [1 x i64] }, align 8
  %0 = bitcast { i8*, i8*, i64, i64, i8*, [1 x i64], [1 x i64] }* %.30 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull %0, i8 0, i64 56, i32 8, i1 false)
  br i1 %.7, label %entry.if, label %entry.endif, !prof !7

entry.if:                                         ; preds = %entry.endif.endif.endif, %entry
  ret i8* null

entry.endif:                                      ; preds = %entry
  %.11 = icmp eq i8* %py_closure, null
  br i1 %.11, label %entry.endif.if, label %entry.endif.endif, !prof !7

entry.endif.if:                                   ; preds = %entry.endif
  %.13 = call i32 @puts(i8* getelementptr inbounds ([38 x i8], [38 x i8]* @".const.Fatal error: missing _dynfunc.Closure", i64 0, i64 0))
  unreachable

entry.endif.endif:                                ; preds = %entry.endif
  %.15 = ptrtoint i8* %py_closure to i64
  %.16 = add i64 %.15, 24
  %.18 = inttoptr i64 %.16 to { i8* }*
  %.1934 = bitcast { i8* }* %.18 to i8**
  %.20 = load i8*, i8** %.1934, align 8
  %.25 = icmp eq i8* %.20, null
  br i1 %.25, label %entry.endif.endif.if, label %entry.endif.endif.endif, !prof !7

entry.endif.endif.if:                             ; preds = %entry.endif.endif
  call void @PyErr_SetString(i8* nonnull @PyExc_RuntimeError, i8* getelementptr inbounds ([20 x i8], [20 x i8]* @".const.missing Environment", i64 0, i64 0))
  ret i8* null

entry.endif.endif.endif:                          ; preds = %entry.endif.endif
  %1 = bitcast { i8*, i8*, i64, i64, i8*, [1 x i64], [1 x i64] }* %.30 to i8**
  %.29 = load i8*, i8** %.5, align 8
  %.32 = bitcast { i8*, i8*, i64, i64, i8*, [1 x i64], [1 x i64] }* %.30 to i8*
  %.33 = call i32 @NRT_adapt_ndarray_from_python(i8* %.29, i8* nonnull %.32)
  %.34 = icmp eq i32 %.33, 0
  %.35.fca.0.load = load i8*, i8** %1, align 8
  br i1 %.34, label %entry.endif.endif.endif.endif, label %entry.if, !prof !8

entry.endif.endif.endif.endif:                    ; preds = %entry.endif.endif.endif
  %2 = bitcast { i8*, i8*, i64, i64, i8*, [1 x i64], [1 x i64] }* %.30 to i8*
  %sunkaddr = getelementptr i8, i8* %2, i64 16
  %3 = bitcast i8* %sunkaddr to i64*
  %.35.fca.2.load = load i64, i64* %3, align 8
  %4 = bitcast { i8*, i8*, i64, i64, i8*, [1 x i64], [1 x i64] }* %.30 to i8*
  %sunkaddr35 = getelementptr i8, i8* %4, i64 32
  %5 = bitcast i8* %sunkaddr35 to i8**
  %.35.fca.4.load = load i8*, i8** %5, align 8
  %.78.i = icmp sgt i64 %.35.fca.2.load, 0
  br i1 %.78.i, label %B16.lr.ph.i, label %"_ZN8__main__9sum2d$241E5ArrayIaLi1E1C7mutable7alignedE.exit"

B16.lr.ph.i:                                      ; preds = %entry.endif.endif.endif.endif
  %6 = xor i64 %.35.fca.2.load, -1
  %7 = icmp sgt i64 %6, -2
  %smax = select i1 %7, i64 %6, i64 -2
  %8 = add i64 %.35.fca.2.load, %smax
  %9 = add i64 %8, 2
  %min.iters.check = icmp ult i64 %9, 32
  br i1 %min.iters.check, label %B16.i.preheader, label %vector.ph

vector.ph:                                        ; preds = %B16.lr.ph.i
  %n.vec = and i64 %9, -32
  %ind.end = sub i64 %.35.fca.2.load, %n.vec
  %10 = add i64 %n.vec, -32
  %11 = lshr exact i64 %10, 5
  %12 = add nuw nsw i64 %11, 1
  %xtraiter = and i64 %12, 3
  %13 = icmp ult i64 %10, 96
  br i1 %13, label %middle.block.unr-lcssa, label %vector.ph.new

vector.ph.new:                                    ; preds = %vector.ph
  %14 = add i64 %xtraiter, -1
  %15 = sub i64 %14, %11
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph.new
  %lsr.iv31 = phi i64 [ %lsr.iv.next32, %vector.body ], [ %15, %vector.ph.new ]
  %index = phi i64 [ 0, %vector.ph.new ], [ %index.next.3, %vector.body ]
  %vec.ind = phi <32 x i64> [ <i64 0, i64 1, i64 2, i64 3, i64 4, i64 5, i64 6, i64 7, i64 8, i64 9, i64 10, i64 11, i64 12, i64 13, i64 14, i64 15, i64 16, i64 17, i64 18, i64 19, i64 20, i64 21, i64 22, i64 23, i64 24, i64 25, i64 26, i64 27, i64 28, i64 29, i64 30, i64 31>, %vector.ph.new ], [ %vec.ind.next.3, %vector.body ]
  %sunkaddr36 = getelementptr i8, i8* %.35.fca.4.load, i64 %index
  %16 = bitcast i8* %sunkaddr36 to <32 x i8>*
  %wide.load = load <32 x i8>, <32 x i8>* %16, align 1, !noalias !9
  %17 = zext <32 x i8> %wide.load to <32 x i64>
  %18 = add <32 x i64> %vec.ind, %17
  %19 = trunc <32 x i64> %18 to <32 x i8>
  %20 = icmp sgt <32 x i8> %19, <i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100>
  %21 = add <32 x i64> %18, <i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156>
  %22 = select <32 x i1> %20, <32 x i64> %21, <32 x i64> %18
  %23 = trunc <32 x i64> %22 to <32 x i8>
  %sunkaddr37 = getelementptr i8, i8* %.35.fca.4.load, i64 %index
  %24 = bitcast i8* %sunkaddr37 to <32 x i8>*
  store <32 x i8> %23, <32 x i8>* %24, align 1, !noalias !9
  %vec.ind.next = add <32 x i64> %vec.ind, <i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32>
  %sunkaddr38 = getelementptr i8, i8* %.35.fca.4.load, i64 %index
  %sunkaddr39 = getelementptr i8, i8* %sunkaddr38, i64 32
  %25 = bitcast i8* %sunkaddr39 to <32 x i8>*
  %wide.load.1 = load <32 x i8>, <32 x i8>* %25, align 1, !noalias !9
  %26 = zext <32 x i8> %wide.load.1 to <32 x i64>
  %27 = add <32 x i64> %vec.ind.next, %26
  %28 = trunc <32 x i64> %27 to <32 x i8>
  %29 = icmp sgt <32 x i8> %28, <i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100>
  %30 = add <32 x i64> %27, <i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156>
  %31 = select <32 x i1> %29, <32 x i64> %30, <32 x i64> %27
  %32 = trunc <32 x i64> %31 to <32 x i8>
  %sunkaddr40 = getelementptr i8, i8* %.35.fca.4.load, i64 %index
  %sunkaddr41 = getelementptr i8, i8* %sunkaddr40, i64 32
  %33 = bitcast i8* %sunkaddr41 to <32 x i8>*
  store <32 x i8> %32, <32 x i8>* %33, align 1, !noalias !9
  %vec.ind.next.1 = add <32 x i64> %vec.ind, <i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64, i64 64>
  %sunkaddr42 = getelementptr i8, i8* %.35.fca.4.load, i64 %index
  %sunkaddr43 = getelementptr i8, i8* %sunkaddr42, i64 64
  %34 = bitcast i8* %sunkaddr43 to <32 x i8>*
  %wide.load.2 = load <32 x i8>, <32 x i8>* %34, align 1, !noalias !9
  %35 = zext <32 x i8> %wide.load.2 to <32 x i64>
  %36 = add <32 x i64> %vec.ind.next.1, %35
  %37 = trunc <32 x i64> %36 to <32 x i8>
  %38 = icmp sgt <32 x i8> %37, <i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100>
  %39 = add <32 x i64> %36, <i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156>
  %40 = select <32 x i1> %38, <32 x i64> %39, <32 x i64> %36
  %41 = trunc <32 x i64> %40 to <32 x i8>
  %sunkaddr44 = getelementptr i8, i8* %.35.fca.4.load, i64 %index
  %sunkaddr45 = getelementptr i8, i8* %sunkaddr44, i64 64
  %42 = bitcast i8* %sunkaddr45 to <32 x i8>*
  store <32 x i8> %41, <32 x i8>* %42, align 1, !noalias !9
  %vec.ind.next.2 = add <32 x i64> %vec.ind, <i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96, i64 96>
  %sunkaddr46 = getelementptr i8, i8* %.35.fca.4.load, i64 %index
  %sunkaddr47 = getelementptr i8, i8* %sunkaddr46, i64 96
  %43 = bitcast i8* %sunkaddr47 to <32 x i8>*
  %wide.load.3 = load <32 x i8>, <32 x i8>* %43, align 1, !noalias !9
  %44 = zext <32 x i8> %wide.load.3 to <32 x i64>
  %45 = add <32 x i64> %vec.ind.next.2, %44
  %46 = trunc <32 x i64> %45 to <32 x i8>
  %47 = icmp sgt <32 x i8> %46, <i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100>
  %48 = add <32 x i64> %45, <i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156>
  %49 = select <32 x i1> %47, <32 x i64> %48, <32 x i64> %45
  %50 = trunc <32 x i64> %49 to <32 x i8>
  %sunkaddr48 = getelementptr i8, i8* %.35.fca.4.load, i64 %index
  %sunkaddr49 = getelementptr i8, i8* %sunkaddr48, i64 96
  %51 = bitcast i8* %sunkaddr49 to <32 x i8>*
  store <32 x i8> %50, <32 x i8>* %51, align 1, !noalias !9
  %index.next.3 = add i64 %index, 128
  %vec.ind.next.3 = add <32 x i64> %vec.ind, <i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128, i64 128>
  %lsr.iv.next32 = add i64 %lsr.iv31, 4
  %niter.ncmp.3 = icmp eq i64 %lsr.iv.next32, 0
  br i1 %niter.ncmp.3, label %middle.block.unr-lcssa, label %vector.body, !llvm.loop !12

middle.block.unr-lcssa:                           ; preds = %vector.body, %vector.ph
  %index.unr = phi i64 [ 0, %vector.ph ], [ %index.next.3, %vector.body ]
  %vec.ind.unr = phi <32 x i64> [ <i64 0, i64 1, i64 2, i64 3, i64 4, i64 5, i64 6, i64 7, i64 8, i64 9, i64 10, i64 11, i64 12, i64 13, i64 14, i64 15, i64 16, i64 17, i64 18, i64 19, i64 20, i64 21, i64 22, i64 23, i64 24, i64 25, i64 26, i64 27, i64 28, i64 29, i64 30, i64 31>, %vector.ph ], [ %vec.ind.next.3, %vector.body ]
  %lcmp.mod = icmp eq i64 %xtraiter, 0
  br i1 %lcmp.mod, label %middle.block, label %vector.body.epil.preheader

vector.body.epil.preheader:                       ; preds = %middle.block.unr-lcssa
  %scevgep2 = getelementptr i8, i8* %.35.fca.4.load, i64 %index.unr
  %52 = sub i64 0, %xtraiter
  br label %vector.body.epil

vector.body.epil:                                 ; preds = %vector.body.epil, %vector.body.epil.preheader
  %lsr.iv6 = phi i64 [ %lsr.iv.next7, %vector.body.epil ], [ %52, %vector.body.epil.preheader ]
  %lsr.iv3 = phi i8* [ %scevgep4, %vector.body.epil ], [ %scevgep2, %vector.body.epil.preheader ]
  %vec.ind.epil = phi <32 x i64> [ %vec.ind.unr, %vector.body.epil.preheader ], [ %vec.ind.next.epil, %vector.body.epil ]
  %lsr.iv35 = bitcast i8* %lsr.iv3 to <32 x i8>*
  %wide.load.epil = load <32 x i8>, <32 x i8>* %lsr.iv35, align 1, !noalias !9
  %53 = zext <32 x i8> %wide.load.epil to <32 x i64>
  %54 = add <32 x i64> %vec.ind.epil, %53
  %55 = trunc <32 x i64> %54 to <32 x i8>
  %56 = icmp sgt <32 x i8> %55, <i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100, i8 100>
  %57 = add <32 x i64> %54, <i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156, i64 156>
  %58 = select <32 x i1> %56, <32 x i64> %57, <32 x i64> %54
  %59 = trunc <32 x i64> %58 to <32 x i8>
  store <32 x i8> %59, <32 x i8>* %lsr.iv35, align 1, !noalias !9
  %vec.ind.next.epil = add <32 x i64> %vec.ind.epil, <i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32>
  %scevgep4 = getelementptr i8, i8* %lsr.iv3, i64 32
  %lsr.iv.next7 = add nsw i64 %lsr.iv6, 1
  %epil.iter.cmp = icmp eq i64 %lsr.iv.next7, 0
  br i1 %epil.iter.cmp, label %middle.block, label %vector.body.epil, !llvm.loop !13

middle.block:                                     ; preds = %vector.body.epil, %middle.block.unr-lcssa
  %cmp.n = icmp eq i64 %9, %n.vec
  br i1 %cmp.n, label %"_ZN8__main__9sum2d$241E5ArrayIaLi1E1C7mutable7alignedE.exit", label %B16.i.preheader

B16.i.preheader:                                  ; preds = %middle.block, %B16.lr.ph.i
  %.68.07.i.ph = phi i64 [ %.35.fca.2.load, %B16.lr.ph.i ], [ %ind.end, %middle.block ]
  %.66.06.i.ph = phi i64 [ 0, %B16.lr.ph.i ], [ %n.vec, %middle.block ]
  %60 = add i64 %.68.07.i.ph, 1
  br label %B16.i

B16.i:                                            ; preds = %B16.i.preheader, %B16.i
  %lsr.iv = phi i64 [ %60, %B16.i.preheader ], [ %lsr.iv.next, %B16.i ]
  %.66.06.i = phi i64 [ %.130.i, %B16.i ], [ %.66.06.i.ph, %B16.i.preheader ]
  %.130.i = add nuw nsw i64 %.66.06.i, 1
  %scevgep = getelementptr i8, i8* %.35.fca.4.load, i64 %.66.06.i
  %.192.i = load i8, i8* %scevgep, align 1, !noalias !9
  %.1981.i = zext i8 %.192.i to i64
  %.199.i = add i64 %.66.06.i, %.1981.i
  %tmp = trunc i64 %.199.i to i8
  %.278.i = icmp sgt i8 %tmp, 100
  %.329.i = add i64 %.199.i, 156
  %.329..199.i = select i1 %.278.i, i64 %.329.i, i64 %.199.i
  %storemerge.i = trunc i64 %.329..199.i to i8
  store i8 %storemerge.i, i8* %scevgep, align 1, !noalias !9
  %lsr.iv.next = add i64 %lsr.iv, -1
  %.117.i = icmp sgt i64 %lsr.iv.next, 1
  br i1 %.117.i, label %B16.i, label %"_ZN8__main__9sum2d$241E5ArrayIaLi1E1C7mutable7alignedE.exit", !llvm.loop !14

"_ZN8__main__9sum2d$241E5ArrayIaLi1E1C7mutable7alignedE.exit": ; preds = %B16.i, %middle.block, %entry.endif.endif.endif.endif
  call void @NRT_decref(i8* %.35.fca.0.load)
  call void @Py_IncRef(i8* nonnull @_Py_NoneStruct)
  ret i8* @_Py_NoneStruct
}

declare i32 @PyArg_UnpackTuple(i8*, i8*, i64, i64, ...) local_unnamed_addr

; Function Attrs: nounwind
declare i32 @puts(i8* nocapture readonly) local_unnamed_addr #1

declare void @PyErr_SetString(i8*, i8*) local_unnamed_addr

declare i32 @NRT_adapt_ndarray_from_python(i8* nocapture, i8* nocapture) local_unnamed_addr

declare void @Py_IncRef(i8*) local_unnamed_addr

; Function Attrs: noinline
define linkonce_odr void @NRT_decref(i8* %.1) local_unnamed_addr #2 {
.3:
  %.4 = icmp eq i8* %.1, null
  br i1 %.4, label %.3.if, label %.3.endif, !prof !7

.3.if:                                            ; preds = %.3.endif, %.3
  ret void

.3.endif:                                         ; preds = %.3
  %.7 = bitcast i8* %.1 to i64*
  %.4.i = atomicrmw sub i64* %.7, i64 1 monotonic
  %.9 = icmp eq i64 %.4.i, 1
  br i1 %.9, label %.3.endif.if, label %.3.if, !prof !7

.3.endif.if:                                      ; preds = %.3.endif
  tail call void @NRT_MemInfo_call_dtor(i8* nonnull %.1)
  ret void
}

declare void @NRT_MemInfo_call_dtor(i8*) local_unnamed_addr

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i32, i1) #3

; Function Attrs: nounwind
declare void @llvm.stackprotector(i8*, i8**) #1

attributes #0 = { norecurse nounwind }
attributes #1 = { nounwind }
attributes #2 = { noinline }
attributes #3 = { argmemonly nounwind }

!0 = distinct !{!0, !1, !2}
!1 = !{!"llvm.loop.vectorize.width", i32 1}
!2 = !{!"llvm.loop.interleave.count", i32 1}
!3 = distinct !{!3, !4}
!4 = !{!"llvm.loop.unroll.disable"}
!5 = distinct !{!5, !6, !1, !2}
!6 = !{!"llvm.loop.unroll.runtime.disable"}
!7 = !{!"branch_weights", i32 1, i32 99}
!8 = !{!"branch_weights", i32 99, i32 1}
!9 = !{!10}
!10 = distinct !{!10, !11, !"_ZN8__main__9sum2d$241E5ArrayIaLi1E1C7mutable7alignedE: %retptr"}
!11 = distinct !{!11, !"_ZN8__main__9sum2d$241E5ArrayIaLi1E1C7mutable7alignedE"}
!12 = distinct !{!12, !1, !2}
!13 = distinct !{!13, !4}
!14 = distinct !{!14, !6, !1, !2}
